{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Organizaci&oacute;n de Datos</center>\n",
    "#### <center>C&aacute;tedra Ing. Rodriguez, Juan Manuel </center>\n",
    "## <center>Trabajo Práctico 2 : Críticas Cinematográficas </center>\n",
    "#### Grupo 29:\n",
    "* Alen Davies Leccese - 107084\n",
    "* Luca Lazcano - 107044"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports y carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold, RandomizedSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, confusion_matrix, classification_report, make_scorer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Datasets/train.csv')\n",
    "test = pd.read_csv('Datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas: 50000 Columnas: 3\n"
     ]
    }
   ],
   "source": [
    "shape = train.shape\n",
    "print(\"Filas: %d Columnas: %d\" % (shape[0], shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos por convertir a tipo str y a minúscula la columna ``'review_es'``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['review_es'] = test['review_es'].astype(str).str.lower()\n",
    "train['review_es'] = train['review_es'].astype(str).str.lower()\n",
    "train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos RegexpTokenizer de nltk para splitear las reviews en palabras. Usamos la expresión regular ``\\w+`` que representa una expresión regular que coincide con una o más letras, números o guiones bajos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp = RegexpTokenizer('\\w+')\n",
    "\n",
    "test['review_token'] = test['review_es'].apply(regexp.tokenize)\n",
    "train['review_token'] = train['review_es'].apply(regexp.tokenize)\n",
    "train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las stopwords (palabras vacías), son palabras muy comunes en un idioma que generalmente no aportan un significado importante al análisis de texto.\n",
    "\n",
    "Las stopwords incluyen palabras como artículos (\"el\", \"la\", \"los\", \"las\"), pronombres (\"yo\", \"tú\", \"él\", \"ella\"), preposiciones (\"a\", \"de\", \"en\", \"con\").\n",
    "\n",
    "Las stopwords se eliminan para reducir el ruido y el tamaño del vocabulario en el texto analizado, centrándose en las palabras clave. Al eliminar estas palabras vacías, se puede mejorar el rendimiento de algoritmos de procesamiento de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_es = nltk.corpus.stopwords.words(\"spanish\")\n",
    "print(stopwords_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['review_token'] = test['review_token'].apply(lambda x: [item for item in x if item not in stopwords_es])\n",
    "train['review_token'] = train['review_token'].apply(lambda x: [item for item in x if item not in stopwords_es])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = nltk.corpus.stopwords.words(\"english\")\n",
    "print(stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['review_token'] = test['review_token'].apply(lambda x: [item for item in x if item not in stopwords_en])\n",
    "train['review_token'] = train['review_token'].apply(lambda x: [item for item in x if item not in stopwords_en])\n",
    "train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrado por largo del palabras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a eliminar palabras poco frecuentes manteniendo solo las palabras que tienen más de 3 letras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['review_token'] = test['review_token'].apply(lambda x: [item for item in x if len(item) > 3])\n",
    "train['review_token'] = train['review_token'].apply(lambda x: [item for item in x if len(item) > 3])\n",
    "train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Igualar palabras con o sin acento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sacar todas las tildes\n",
    "def remove_tildes(text):\n",
    "    tildes = {\n",
    "        'á': 'a',\n",
    "        'é': 'e',\n",
    "        'í': 'i',\n",
    "        'ó': 'o',\n",
    "        'ú': 'u'\n",
    "    }\n",
    "    for key, value in tildes.items():\n",
    "        text = text.replace(key, value)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['review_token'] = test['review_token'].apply(lambda x: [remove_tildes(item) for item in x])\n",
    "train['review_token'] = train['review_token'].apply(lambda x: [remove_tildes(item) for item in x])\n",
    "train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de vocabulario"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: explicar esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario = Counter()\n",
    "\n",
    "for review in train['review_token']:\n",
    "    for palabra in review:\n",
    "        vocabulario[palabra] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "vocabulario_truncado = [ word for word, count in vocabulario.most_common()[:vocab_size] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulario_truncado)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafico las 20 palabras más frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar las palabras más frecuentes con vocabulario_truncado.most_common(20)\n",
    "\n",
    "top20 = pd.Series(dict(vocabulario.most_common(20)))\n",
    "sns.barplot(y=top20.index, x=top20.values).set(title='Top 20 palabras más frecuentes', xlabel='Frecuencia', ylabel='Palabra')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrado de las que no están en el vocabulario truncado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['review_filtrado'] = test['review_token'].apply(lambda x: [item for item in x if item in vocabulario_truncado])\n",
    "train['review_filtrado'] = train['review_token'].apply(lambda x: [item for item in x if item in vocabulario_truncado])\n",
    "train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncado de las reviews"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acorto las reviews a las 300 primeras palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardo las 300 primeras palabras de cada review\n",
    "test['review_filtrado_300'] = test['review_filtrado'].apply(lambda x: x[:300])\n",
    "train['review_filtrado_300'] = train['review_filtrado'].apply(lambda x: x[:300])\n",
    "train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unión de review filtrado en string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all words of each review into a string\n",
    "test['review_filtrado_string'] = test['review_filtrado_300'].apply(lambda x: ' '.join(x))\n",
    "train['review_filtrado_string'] = train['review_filtrado_300'].apply(lambda x: ' '.join(x))\n",
    "train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La técnica Bag of Words es una forma de representar datos de texto en aprendizaje automático. El concepto básico es tratar cada documento de texto como una \"bolsa\" de palabras, donde se ignora el orden y la estructura gramatical de las palabras. En lugar de eso, se enfoca únicamente en la presencia y frecuencia de las palabras en el texto."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar ``TfidfVectorizer`` de scikit-learn que implementa la técnica de ponderación TF-IDF (Term Frequency-Inverse Document Frequency) para convertir datos de texto en características numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_fid = vectorizer.fit_transform(train.review_filtrado_string)\n",
    "test_fid = vectorizer.transform(test.review_filtrado_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_fid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train/test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dvidimos el dataset en train y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train_fid,\n",
    "                                                    train.sentimiento, \n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=RANDOM_STATE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Naive"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos optimización de hiperparámetros con Random Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cantidad de combinaciones que quiero probar\n",
    "n=15\n",
    "\n",
    "#Grilla de Parámetros a probar\n",
    "params_grid={ 'alpha': [0.5, 1.0, 2.0, 5.0],\n",
    "               'class_prior': [None, [0.1, 0.9], [0.2, 0.8], [0.3, 0.7], [0.4, 0.6], [0.5, 0.5], [0.6, 0.4]],\n",
    "               'fit_prior': [True, False],\n",
    "               'force_alpha': [True, False]\n",
    "             }\n",
    "                \n",
    "#Cantidad de splits para el Cross Validation\n",
    "folds=5\n",
    "\n",
    "#Kfold estratificado\n",
    "kfoldcv = StratifiedKFold(n_splits=folds, shuffle=True)\n",
    "\n",
    "#Clasificador\n",
    "NB_model = MultinomialNB()\n",
    "\n",
    "# Metrica que quiero optimizar F1 Score\n",
    "scorer_fn = make_scorer(sk.metrics.f1_score, pos_label='positivo')\n",
    "\n",
    "#Random Search Cross Validation\n",
    "rand_NB = RandomizedSearchCV(estimator = NB_model,\n",
    "                              param_distributions = params_grid,\n",
    "                              scoring = scorer_fn,\n",
    "                              cv=kfoldcv,\n",
    "                              n_iter = n,\n",
    "                              random_state = RANDOM_STATE) \n",
    "\n",
    "rand_NB.fit(x_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos cuales fueron los mejores hiperparámetros y la mejor métrica obtenida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mejores parámetros: {}'.format(rand_NB.best_params_))\n",
    "print('Mejor métrica: {}'.format(rand_NB.best_score_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo de Bayes Naive con los mejores hiperparámetros y realizamos las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_model = MultinomialNB().set_params(**rand_NB.best_params_)\n",
    "NB_model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = NB_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best Training Accuracy: {NB_model.score(x_train, y_train)}')\n",
    "print(f'Best Testing Accuracy: {NB_model.score(x_test, y_test)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos las métricas de accuracy, precision, recall y f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label='positivo')\n",
    "f1 = f1_score(y_test, y_pred, pos_label='positivo')\n",
    "precision = precision_score(y_test, y_pred, pos_label='positivo')\n",
    "\n",
    "print(\"Accuracy: \"+str(accuracy))\n",
    "print(\"Recall: \"+str(recall))\n",
    "print(\"Precision: \"+str(precision))\n",
    "print(\"f1 score: \"+str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reporte de Clasificación\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz de Confusión\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "sns.heatmap(cm, cmap='Blues',annot=True,fmt='g').set(title='Matriz de Confusión', xlabel='Predicted', ylabel='True', xticklabels=['negativo', 'positivo'], yticklabels=['negativo', 'positivo'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKaIr7rlv-rn"
   },
   "outputs": [],
   "source": [
    "pickle.dump(NB_model, open('NB_grupo29.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kp4RPtwB_mXH"
   },
   "outputs": [],
   "source": [
    "X_submission_NB = test_fid\n",
    "y_pred_test_NB = NB_model.predict(X_submission_NB)\n",
    "df_submission_NB = pd.DataFrame({'ID': test['ID'], 'sentimiento': y_pred_test_NB})\n",
    "df_submission_NB.to_csv('NB03.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'negativo': 0, 'positivo': 1}\n",
    "y_train = y_train.map(label_map)\n",
    "y_test  = y_test.map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cantidad de combinaciones que quiero probar\n",
    "n=5\n",
    "\n",
    "#Grilla de Parámetros\n",
    "params_grid={ 'n_estimators': [100, 200, 300, 400, 500],\n",
    "                'min_samples_split': [2, 5, 10, 15, 20, 25, 30, 35, 40],\n",
    "                'max_depth': [None],\n",
    "                'max_samples': [None, 0.5, 0.75, 0.9, 0.95, 0.99, 1.0],\n",
    "                'ccp_alpha': [0.0, 0.001, 0.01, 0.1, 1.0],\n",
    "             }\n",
    "\n",
    "#Clasificador\n",
    "rf_model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "# Metrica que quiero optimizar F1 Score\n",
    "scorer_fn = make_scorer(sk.metrics.f1_score)\n",
    "\n",
    "#Random Search Cross Validation\n",
    "rand_rf = RandomizedSearchCV(estimator = rf_model,\n",
    "                              param_distributions = params_grid,\n",
    "                              scoring = scorer_fn,\n",
    "                              n_iter = n,\n",
    "                              random_state=RANDOM_STATE) \n",
    "\n",
    "rand_rf.fit(x_train, y_train)\n",
    "rand_rf.cv_results_['mean_test_score'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mejores parámetros: {}'.format(rand_rf.best_params_))\n",
    "print('Mejor métrica: {}'.format(rand_rf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(random_state=RANDOM_STATE).set_params(**rand_rf.best_params_)\n",
    "rf_model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best Training Accuracy: {rf_model.score(x_train, y_train)}')\n",
    "print(f'Best Testing Accuracy: {rf_model.score(x_test, y_test)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label='positivo')\n",
    "f1 = f1_score(y_test, y_pred, pos_label='positivo')\n",
    "precision = precision_score(y_test, y_pred, pos_label='positivo')\n",
    "\n",
    "print(\"Accuracy: \"+str(accuracy))\n",
    "print(\"Recall: \"+str(recall))\n",
    "print(\"Precision: \"+str(precision))\n",
    "print(\"f1 score: \"+str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reporte de Clasificación\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz de Confusión\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "sns.heatmap(cm, cmap='Blues',annot=True,fmt='g').set(title='Matriz de Confusión', xlabel='Predicted', ylabel='True', xticklabels=['negativo', 'positivo'], yticklabels=['negativo', 'positivo'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKaIr7rlv-rn"
   },
   "outputs": [],
   "source": [
    "pickle.dump(rf_model, open('rf_grupo29.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kp4RPtwB_mXH"
   },
   "outputs": [],
   "source": [
    "X_submission_rf = test_fid\n",
    "y_pred_test_rf = rf_model.predict(X_submission_rf)\n",
    "y_pred_test_rf\n",
    "df_submission_rf = pd.DataFrame({'ID': test['ID'], 'sentimiento': y_pred_test_rf})\n",
    "df_submission_rf.to_csv('rf01.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'negativo': 0, 'positivo': 1}\n",
    "y_train_mapped = y_train.map(label_map)\n",
    "y_test_mapped = y_test.map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cantidad de combinaciones que quiero probar\n",
    "n=5\n",
    "\n",
    "#Grilla de Parámetros\n",
    "params_grid= {'learning_rate': np.linspace(0.05, 0.5, 50),\n",
    "                'gamma': [0,1,2],\n",
    "                'max_depth': list(range(2,10)),\n",
    "                'subsample': np.linspace(0, 1, 20),\n",
    "                'lambda': [0,1,2],\n",
    "                'alpha' : [1],\n",
    "                'n_estimators': list(range(10,161,10))\n",
    "              }\n",
    "\n",
    "#Clasificador\n",
    "xgb_model = XGBClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "# Metrica que quiero optimizar F1 Score\n",
    "scorer_fn = make_scorer(sk.metrics.f1_score)\n",
    "\n",
    "#Random Search Cross Validation\n",
    "rand_xgb = RandomizedSearchCV(estimator = xgb_model,\n",
    "                              param_distributions = params_grid,\n",
    "                              scoring = scorer_fn,\n",
    "                              n_iter = n,\n",
    "                              random_state=RANDOM_STATE) \n",
    "\n",
    "rand_xgb.fit(x_train, y_train_mapped)\n",
    "rand_xgb.cv_results_['mean_test_score'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mejores parámetros: {}'.format(rand_xgb.best_params_))\n",
    "print('Mejor métrica: {}'.format(rand_xgb.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(random_state=RANDOM_STATE).set_params(**rand_xgb.best_params_)\n",
    "xgb_model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = xgb_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best Training Accuracy: {xgb_model.score(x_train, y_train)}')\n",
    "print(f'Best Testing Accuracy: {xgb_model.score(x_test, y_test)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test_mapped, y_pred)\n",
    "recall = recall_score(y_test_mapped, y_pred)\n",
    "f1 = f1_score(y_test_mapped, y_pred)\n",
    "precision = precision_score(y_test_mapped, y_pred)\n",
    "\n",
    "print(\"Accuracy: \"+str(accuracy))\n",
    "print(\"Recall: \"+str(recall))\n",
    "print(\"Precision: \"+str(precision))\n",
    "print(\"f1 score: \"+str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reporte de Clasificación\n",
    "print(classification_report(y_test_mapped, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz de Confusión\n",
    "cm = confusion_matrix(y_test_mapped, y_pred)\n",
    "sns.heatmap(cm, cmap='Blues',annot=True,fmt='g').set(title='Matriz de Confusión', xlabel='Predicted', ylabel='True', xticklabels=['negativo', 'positivo'], yticklabels=['negativo', 'positivo'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKaIr7rlv-rn"
   },
   "outputs": [],
   "source": [
    "pickle.dump(xgb_model, open('xgb_grupo29.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kp4RPtwB_mXH"
   },
   "outputs": [],
   "source": [
    "X_submission_xgb = test_fid\n",
    "y_pred_test_xgb = xgb_model.predict(X_submission_xgb)\n",
    "\n",
    "label_map = {0: 'negativo', 1: 'positivo'}\n",
    "y_pred_test_xgb = y_pred_test_xgb.map(label_map)\n",
    "\n",
    "df_submission_xgb = pd.DataFrame({'ID': test['ID'], 'sentimiento': y_pred_test_xgb})\n",
    "df_submission_xgb.to_csv('xgb00.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red Neuronal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tokenizer hace todo el preproeso que ya hicimos, entones lo entrenamos con la review cruda, seteando acá el tamaño del vocabulario, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "REVIEW_MAX_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the words in the training set\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(train['review_es'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cantidad de palabras únicas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos algunas palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['punto humorístico']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[200, 5000]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y los indices de un string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 12, 9]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['la película es malarda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192669\n"
     ]
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index)\n",
    "print(max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array to regular array\n",
    "reviews_lista_train = pd.Series(train['review_es']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Uno de los otros críticos ha mencionado que después de ver solo 1 Oz Episodio, estará enganchado. Tienen razón, ya que esto es exactamente lo que sucedió conmigo. La primera cosa que me golpeó sobre Oz fue su brutalidad y sus escenas de violencia inconfiadas, que se encuentran a la derecha de la palabra. Confía en mí, este no es un espectáculo para los débiles de corazón o tímido. Este espectáculo no extrae punzones con respecto a las drogas, el sexo o la violencia. Es Hardcore, en el uso clásico de la palabra. Se llama OZ, ya que es el apodo dado al Penitenciario del Estado de Seguridad Máximo de Oswald. Se centra principalmente en la ciudad de Emeralda, una sección experimental de la prisión donde todas las células tienen frentes de vidrio y se enfrentan hacia adentro, por lo que la privacidad no es alta en la agenda. Em City es el hogar de muchos ... Fariarios, musulmanes, gangstas, latinos, cristianos, italianos, irlandeses y más ... así que las esposas, las miradas de muerte, las relaciones peligrosas y los acuerdos sombreados nunca están lejos. Yo diría el principal atractivo de El espectáculo se debe al hecho de que va donde otros espectáculos no se atreverían. Olvídate de las imágenes bonitas pintadas para las audiencias convencionales, olvidan el encanto, olviden el romance ... Oz no se mete. El primer episodio que he visto me sorprendió tan desagradable que fue surrealista, no podía decir que estaba listo para ello, pero cuando observé más, desarrollé un gusto por Oz, y me acostumbré a los altos niveles de violencia gráfica. No solo la violencia, sino la injusticia (Guardias torcidas que se vendrán por un níquel, los reclusos que se matarán en orden y se alejarán con él, de manera educada, los reclusos de clase media se convirtieron en perras de la prisión debido a su falta de habilidades callejeras O experiencia en la prisión) viendo oz, puede sentirse cómodo con lo que es incómodo visualización ... eso es si puedes ponerte en contacto con tu lado más oscuro.'\n",
      " 'Una pequeña pequeña producción.La técnica de filmación es muy incuestionable, muy antigua, la moda de la BBC y le da una sensación de realismo reconfortante, y, a veces, incómodo, y, a veces, a la pieza.Los actores son extremadamente bien elegidos, Michael Sheen, no solo \"tiene todo el polari\", ¡pero tiene todas las voces por palmaditas!Realmente puede ver la edición perfecta guiada por las referencias a las entradas del diario de Williams, no solo vale la pena la observación, pero es una pieza imperrementemente escrita y realizada.Una producción magistral sobre uno de los grandes maestros de la comedia y su vida.El realismo realmente llega a casa con las pequeñas cosas: la fantasía del guardia que, en lugar de usar las técnicas de \"sueño\" tradicionales permanece sólido, entonces desaparece.Se desempeña nuestro conocimiento y nuestros sentidos, particularmente con las escenas relacionadas con Orton y Halliwell y los conjuntos (particularmente de su apartamento con murales de Halliwell que decoran cada superficie) están terriblemente bien hechos.'\n",
      " 'Pensé que esta era una manera maravillosa de pasar tiempo en un fin de semana de verano demasiado caliente, sentado en el teatro con aire acondicionado y observando una comedia alegre.La parcela es simplista, pero el diálogo es ingenioso y los personajes son agradables (incluso el asesino en serie de la sospecha de pan del pozo).Mientras que algunos pueden decepcionarse cuando se dan cuenta de que este no es el punto de partido 2: la adicción al riesgo, pensé que era una prueba de que Woody Allen todavía está en control del estilo que muchos de nosotros hemos crecido a amar. Esto fue el más reído.En una de las comedias de Woody en años (¡Dare, digo una década?).Mientras nunca me quedé impresionado con Scarlet Johanson, en esto logró tonificar su imagen \"sexy\" y saltó a una joven promedio, pero enérgica. Esto puede que no sea la joya de la corona de su carrera, pero fue Wittier.\"El diablo viste prada\" y más interesante que \"Superman\" una gran comedia para ir a ver con amigos.']\n"
     ]
    }
   ],
   "source": [
    "print(reviews_lista_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train = tokenizer.texts_to_sequences(reviews_lista_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_truncado = tf.keras.preprocessing.sequence.pad_sequences(encoded_train, maxlen=REVIEW_MAX_LENGTH, truncating='post', padding='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 80   1  11 182  70   3  77   1  46  35 276 395 201 249  74   3  39   9\n",
      "  605  16]\n",
      " [ 10 379 379 349   2   1   9  40  40   2   1   2   4  58 258  10 648   1\n",
      "    4   7]\n",
      " [292   3  21  48  10 118 825   1 621  73   5   8 659   1 931   1 131   5\n",
      "    6 617]\n",
      " [724  51  10 267 125   8 226 417   3  51   8   5  19   4  38 684  99  52\n",
      "    6  73]\n",
      " [  6 211   5   6  73   1   9  10  12 723  15  46   6 489 212 852   8  54\n",
      "   17 870]]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_train_truncado[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_train_ragged_tensor = tf.ragged.stack([tf.convert_to_tensor(arr) for arr in encoded_train], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50000, 1798), dtype=int32, numpy=\n",
       "array([[  80,    1,   11, ...,    0,    0,    0],\n",
       "       [  10,  379,  379, ...,    0,    0,    0],\n",
       "       [ 292,    3,   21, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 320, 6286,    5, ...,    0,    0,    0],\n",
       "       [ 973,    7,  190, ...,    0,    0,    0],\n",
       "       [ 315,  998,    3, ...,    0,    0,    0]])>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoded_train_dense_tensor = encoded_train_ragged_tensor.to_tensor()\n",
    "# encoded_train_dense_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't convert non-rectangular Python sequence to Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m encoded_train_tensor \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mconvert_to_tensor(encoded_train, dtype\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mfloat32)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    102\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor."
     ]
    }
   ],
   "source": [
    "# encoded_train_tensor = tf.convert_to_tensor(encoded_train, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luca\\AppData\\Local\\Temp\\ipykernel_21456\\3483425166.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.array(encoded_train).flatten()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([list([80, 1, 11, 182, 1238, 70, 3180, 3, 77, 1, 46, 35, 276, 3713, 395, 2459, 6048, 201, 249, 74, 3, 39, 9, 605, 16, 3, 1009, 2279, 2, 145, 217, 3, 29, 2361, 54, 3713, 31, 19, 6605, 4, 38, 116, 1, 568, 3, 14, 703, 7, 2, 2153, 1, 2, 708, 3974, 5, 235, 33, 13, 9, 8, 228, 15, 11, 2404, 1, 447, 32, 6049, 33, 228, 13, 18, 791, 7, 17, 895, 6, 484, 32, 2, 568, 9, 4535, 5, 6, 677, 579, 1, 2, 708, 14, 1157, 3713, 74, 3, 9, 6, 607, 27, 25, 242, 1, 1454, 2663, 1, 14, 2904, 789, 5, 2, 288, 1, 10, 2908, 6144, 1, 2, 1620, 125, 169, 17, 7389, 201, 1, 5695, 4, 14, 3631, 341, 20, 16, 3, 2, 13, 9, 652, 5, 2, 5495, 2571, 9, 6, 1300, 1, 185, 8152, 4399, 4333, 9429, 4, 24, 92, 3, 17, 4973, 17, 5326, 1, 310, 17, 870, 9876, 4, 11, 94, 99, 563, 293, 1614, 6, 250, 1395, 1, 6, 228, 14, 170, 27, 81, 1, 3, 298, 125, 182, 1433, 13, 14, 5982, 1, 17, 380, 3992, 15, 17, 1309, 8209, 6, 1477, 6, 932, 3713, 13, 14, 6, 316, 395, 3, 59, 89, 29, 885, 44, 1178, 3, 31, 2934, 13, 469, 115, 3, 72, 2691, 15, 809, 23, 41, 2059, 24, 8, 1236, 20, 3713, 4, 29, 7, 11, 2578, 1919, 1, 568, 5001, 13, 35, 2, 568, 280, 2, 8955, 6656, 3, 14, 20, 8, 11, 3, 14, 5, 2233, 4, 14, 18, 69, 1, 118, 11, 1, 548, 712, 14, 4786, 5, 1, 2, 1620, 325, 7, 19, 457, 1, 1601, 32, 591, 5, 2, 1620, 492, 3713, 68, 2509, 6258, 18, 16, 3, 9, 2724, 2024, 65, 9, 30, 314, 5, 2745, 18, 422, 337, 24, 984]),\n",
       "       list([10, 379, 379, 349, 2, 2536, 1, 2071, 9, 40, 40, 1171, 2, 1757, 1, 2, 2395, 4, 58, 258, 10, 648, 1, 2060, 4, 7, 158, 2724, 4, 7, 158, 7, 2, 529, 11, 136, 36, 562, 60, 9033, 453, 5028, 13, 35, 45, 52, 6, 803, 45, 169, 17, 2413, 20, 50, 68, 46, 2, 734, 1101, 20, 17, 2051, 7, 17, 7044, 25, 4511, 1, 1820, 13, 35, 338, 2, 222, 2, 3918, 23, 9, 10, 529, 1586, 4, 3857, 10, 349, 5696, 54, 80, 1, 11, 271, 3156, 1, 2, 184, 4, 19, 84, 6, 2060, 50, 957, 7, 187, 18, 17, 1527, 126, 2, 1087, 25, 3563, 3, 5, 96, 1, 871, 17, 3047, 1, 1163, 6915, 3256, 2136, 173, 4569, 14, 4295, 738, 1996, 4, 1102, 2698, 834, 18, 17, 116, 5442, 18, 4, 4, 11, 1337, 834, 1, 19, 2073, 18, 1, 3, 124, 2725, 99, 1592, 60, 1117]),\n",
       "       list([292, 3, 21, 48, 10, 118, 825, 1, 621, 73, 5, 8, 659, 1, 931, 1, 1628, 131, 1396, 2052, 5, 6, 617, 18, 1048, 4, 1340, 10, 184, 2547, 2, 264, 9, 5959, 23, 6, 263, 9, 2543, 4, 11, 85, 36, 1261, 86, 6, 439, 5, 176, 1, 2, 4655, 1, 3769, 25, 5419, 119, 3, 100, 236, 41, 14, 790, 219, 1, 3, 33, 13, 9, 6, 200, 1, 1608, 207, 2, 6185, 27, 4136, 292, 3, 48, 10, 1314, 1, 3, 3282, 1912, 223, 28, 5, 1231, 25, 358, 3, 185, 1, 646, 1253, 4614, 7, 2160, 39, 31, 6, 24, 5, 10, 1, 17, 1196, 1, 3282, 5, 78, 1637, 10, 477, 119, 94, 29, 3059, 2658, 18, 5, 39, 1662, 19, 433, 1248, 4, 7894, 7, 10, 246, 952, 23, 39, 68, 3, 13, 128, 2, 4121, 1, 2, 1, 19, 437, 23, 31, 6, 2042, 3798, 4, 24, 245, 3, 2874, 10, 64, 184, 15, 387, 7, 46, 18, 322]),\n",
       "       ...,\n",
       "       list([320, 6286, 5, 6006, 20, 20, 11, 5, 2, 400, 990, 4, 2, 1886, 223, 320, 8, 23, 13, 355, 3711, 8, 194, 5, 11, 404, 1, 2, 1704, 79, 13, 132, 3152, 126, 160, 132, 3152, 2428, 35, 79, 2, 1704, 29, 282, 92, 3, 1498, 7, 2, 12, 9, 230, 79, 90, 87, 36, 20, 21, 8311, 3, 14, 609, 3, 9, 10, 2364, 22, 2, 1, 10, 1859, 4809, 2139, 13, 51, 184, 5, 65, 4, 2, 2364, 13, 14, 71, 60, 20, 2, 103, 54, 3808, 2295, 94, 202, 2, 505, 23, 30, 165, 40, 459, 1, 274, 34, 173, 68, 42, 107, 5, 316, 96, 292, 3, 2, 2164, 97, 42, 10, 6388, 4, 2, 145, 4792, 1, 52, 8, 1716, 20, 2, 2695, 2139, 1, 11, 687, 860, 9178, 22, 8, 624, 1, 2, 1133, 155, 3414, 1, 2, 813, 2815, 23, 1201, 3, 17, 4416, 255, 661, 4, 2, 43, 70, 1841, 2, 1841, 1, 2, 897, 1, 2, 84, 303, 1, 11, 90, 2768, 1440, 2, 84, 1, 11, 4157, 2, 84, 1, 2, 362, 25, 1750, 2, 1841, 1, 5594, 54, 6, 211, 7, 17, 87, 2, 1841, 1, 2, 2612, 3, 9080, 2, 674, 2100, 5, 497, 39, 9, 16, 3, 28, 153, 18, 6, 1, 416, 4, 6, 4, 6, 7651, 1, 4607]),\n",
       "       list([973, 7, 190, 3, 221, 5, 4183, 18, 6, 836, 927, 4, 6, 337, 18, 5, 33, 354, 21, 9, 10, 460, 4821, 1335, 9442, 9469, 3, 4, 4398, 3, 1073, 1167, 5, 19, 442, 1474, 25, 2466, 25, 2030, 3, 14, 4, 14, 20, 2, 2795, 1, 11, 542, 6829, 58, 909, 221, 5, 2, 3495, 1, 7404, 4, 26, 8934, 3788, 23, 1453, 1, 2, 4, 6, 1477, 1, 193, 12, 2, 345, 1, 4128, 9045, 9, 4, 11, 706, 886, 179, 32, 101, 36, 8, 454, 18, 252, 6383, 20, 91, 1, 6053, 4, 1329, 6419, 197, 29, 258, 8, 3440, 30, 314, 20, 82, 14, 1854, 5, 3493, 1, 2, 737, 88, 3, 14, 5, 52, 6, 2467, 1, 2, 113, 1, 2, 2110, 4, 2, 2223, 1, 2, 239, 6726, 488, 13, 3572, 701, 8758, 9, 40, 1178, 74, 3, 2, 712, 2512, 8108, 1, 7377, 7, 2, 1355, 23, 169, 38, 99, 36, 22, 11, 1, 15, 42, 80, 20, 80, 7, 432, 3, 2, 43, 14, 341, 1174, 453, 58, 258, 7, 80, 1, 38, 353, 5825, 23, 5, 33, 354, 31, 3924, 74, 3, 19, 1, 358, 2012, 2, 1, 3, 6, 6686, 68]),\n",
       "       list([315, 998, 3, 17, 34, 1, 873, 2526, 686, 2513, 1872, 23, 11, 438, 3863, 10, 12, 3, 128, 44, 111, 22, 100, 1, 11, 198, 663, 572, 21, 12, 625, 10, 161, 3, 29, 1008, 39, 9, 18, 66, 2, 209, 1, 17, 3615, 154, 141, 86, 2, 2099, 1, 46, 7, 11, 2981, 85, 5, 138, 12, 13, 68, 9945, 21, 12, 5422, 17, 116, 1, 8506, 18, 4496, 5453, 4, 7616, 5, 37, 105, 21, 12, 13, 338, 2, 222, 798, 37, 105, 338, 2, 222, 291, 15, 6, 454, 448, 3, 540, 46, 169, 17, 34, 1741, 21, 12, 9, 54, 2, 283, 196, 5, 3, 2, 3071, 86, 11, 4370, 20, 1925, 21, 12])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.array(encoded_train).flatten()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_rnn, x_test_rnn, y_train_rnn, y_test_rnn = train_test_split(encoded_train_truncado,\n",
    "                                                                    train.sentimiento,\n",
    "                                                                    test_size=0.3,\n",
    "                                                                    random_state=RANDOM_STATE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'negativo': 0, 'positivo': 1}\n",
    "y_train_rnn = y_train_rnn.map(label_map)\n",
    "y_test_rnn  = y_test_rnn.map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(35000, 20), dtype=int32, numpy=\n",
       "array([[177,  22,  29, ...,   7,  37,   7],\n",
       "       [ 33,  31,   8, ...,   7, 621,  20],\n",
       "       [ 13, 329,  17, ...,   4,  13, 135],\n",
       "       ...,\n",
       "       [ 39,   9,  10, ..., 210, 831,  52],\n",
       "       [ 21, 349,  31, ...,  72, 149,  11],\n",
       "       [ 21,   9,  10, ..., 157,   1,  73]])>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_rnn_ragged = tf.ragged.stack([tf.convert_to_tensor(lista) for lista in x_train_rnn], axis=0)\n",
    "x_train_rnn_dense = x_train_rnn_ragged.to_tensor()\n",
    "x_train_rnn_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([177, 22, 29, 3216, 11, 5813, 13, 913, 21, 12, 2, 887, 1, 3, 14, 97, 2204, 10, 4, 3471, 1, 7, 3896, 37, 2285, 7, 164, 1303, 7, 16, 339, 25, 343, 45, 3, 221, 7, 2, 2016, 1, 17, 398, 1, 264, 8049, 90, 5891, 1, 2, 3, 6858, 6, 3, 7, 11, 1, 8, 337, 7, 164, 4, 7, 330, 761, 4, 7542, 2, 251, 147, 3, 588, 52, 65, 9, 8, 194, 624, 28, 63, 5, 19, 1, 4, 444, 8, 194, 643, 22, 19, 611, 1740, 4, 60, 1966, 9, 3106, 22, 6, 392, 4, 6, 417, 392, 390, 7, 43, 918, 164, 1048]),\n",
       "       list([33, 31, 8, 40, 194, 23, 22, 3421, 409, 2798, 78, 77, 2, 6325, 1230, 6810, 100, 1, 11, 6810, 255, 3991, 13, 973, 7, 621, 20, 124, 1608, 5, 2, 5480, 79, 8067, 15, 156, 37, 105, 11, 535, 1025, 24, 1618, 6, 1291, 2044, 15, 2, 3, 2145, 11, 25, 611, 1, 1, 4, 6193, 26, 1796, 6, 316, 1608, 1, 18, 4, 2066, 1957, 4, 6, 5033, 4, 6, 316, 3275, 130, 6, 561, 5762, 4, 6874, 6302, 100, 6810, 234, 10, 649, 897, 1, 73, 27, 445, 3, 11, 1957, 2, 4860, 72, 7, 35, 409, 1406, 1574, 483, 1957, 4006, 48, 4, 1165, 1957, 48, 131, 1041, 2845, 742, 1957, 4, 2730, 1957, 3822, 2672, 2741, 234, 40, 381, 1223, 483]),\n",
       "       list([13, 329, 17, 519, 5872, 9, 6, 369, 1, 53, 5, 21, 184, 6592, 1, 108, 13, 1673, 82, 115, 4, 13, 1673, 135, 2685, 20, 16, 3, 14, 5, 2, 2957, 5246, 1, 17, 4730, 7166, 7700, 572, 53, 14, 703, 18, 1089, 1, 3860, 5, 96, 1, 87, 661, 2, 43, 344, 7, 10, 2916, 3481, 1, 272, 3371, 2154, 6, 3398, 5266, 5, 38, 782, 6, 1, 4730, 172, 4, 8, 7943, 83, 1, 2158, 1478, 2293, 100, 1, 11, 2951, 1, 17, 4839, 1874, 79, 52, 9, 44, 869, 1614, 3, 22, 1772, 9, 8, 3153, 22, 184, 2, 175, 1, 17, 158, 9, 131, 204, 15, 42, 50, 204, 5467, 154, 3263, 14, 3325, 1, 2, 2065, 1, 2, 4571, 22, 6, 27, 3338, 1549, 64, 1349, 5, 61, 55, 2, 2903, 1, 404, 3921, 14, 256, 18, 6, 2453, 1, 11, 3597, 1, 2, 2013, 4680, 8, 1005, 1, 4, 23, 1, 382, 5467, 1489, 55, 15, 8937, 22, 2, 2127, 1, 211, 4, 22, 1396, 181, 6, 3817, 1, 2, 2628, 7, 167, 25, 2504, 6154, 5141, 1, 2]),\n",
       "       ...,\n",
       "       list([39, 9, 10, 12, 335, 173, 995, 30, 9, 40, 1130, 52, 16, 3, 9, 9, 98, 1307, 37, 210, 831, 52, 6, 1159, 18, 2, 2772, 1, 4, 453, 65, 31, 16, 24, 388, 3, 503, 95, 7, 2, 176, 308, 82, 13, 2887, 815, 2, 43, 1, 453, 8, 1003, 14, 609, 3, 9, 6, 657, 91, 1, 312, 1383, 796, 6691, 7, 497, 20, 2245, 3, 2245, 318, 1047, 7122, 23, 1, 53, 523, 30, 1013, 21, 12, 13, 2773, 24, 3, 11, 1307, 37, 210, 4, 720]),\n",
       "       list([21, 349, 31, 93, 1020, 15, 235, 385, 29, 762, 4139, 34, 7, 1054, 1, 11, 886, 78, 23, 13, 72, 4317, 149, 11, 706, 2476, 179, 1, 21, 43, 30, 20, 7411, 13, 628, 4265, 5, 2, 145, 449, 297, 15, 6, 83, 20, 266, 2240, 783, 6, 3375, 8157, 22, 3593, 10, 591, 1, 2024, 1464, 23, 132, 3, 2, 25, 1399, 195, 13, 28, 1714, 5, 410, 237, 223, 7120, 1197, 8716]),\n",
       "       list([21, 9, 10, 12, 647, 110, 67, 7, 73, 15, 235, 559, 186, 8461, 18, 210, 1264, 4, 1068, 5, 157, 2348, 1, 73, 2383, 6352, 4, 6, 164, 143, 250, 268, 8, 64, 123, 5, 21, 12, 17, 1068, 155, 3590, 255, 2642, 4, 7, 16, 339, 1, 2, 12, 13, 9, 10, 12, 1, 186, 1632, 1, 324, 1632, 1837, 23, 31, 10, 12, 40, 4640, 20, 6, 1687, 62, 306, 31, 391, 20, 21, 12, 21, 12, 9, 10, 12, 1, 186, 3558, 5217, 18, 64, 103, 43, 4, 345, 1188, 21, 12, 6, 943, 25, 143, 1, 11, 85, 48, 107, 4, 133, 71, 527, 3, 5, 168, 295, 492, 8, 1904, 172, 3, 14, 28, 4961, 7, 1180, 325, 7, 3, 21, 12, 132, 3, 14, 129, 18, 8, 350, 24, 1456, 2, 103, 4, 2, 436, 255, 66, 24, 2513])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_train_rnn_np = np.asarray(x_train_rnn, dtype=object)\n",
    "# x_train_rnn_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x_train_rnn_np \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mconvert_to_tensor(x_train_rnn_np)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(x_train_rnn_np)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    102\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "# x_train_rnn_np = tf.convert_to_tensor(x_train_rnn_np)\n",
    "# print(x_train_rnn_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([177, 22, 29, 3216, 11, 5813, 13, 913, 21, 12, 2, 887, 1, 3, 14, 97, 2204, 10, 4, 3471, 1, 7, 3896, 37, 2285, 7, 164, 1303, 7, 16, 339, 25, 343, 45, 3, 221, 7, 2, 2016, 1, 17, 398, 1, 264, 8049, 90, 5891, 1, 2, 3, 6858, 6, 3, 7, 11, 1, 8, 337, 7, 164, 4, 7, 330, 761, 4, 7542, 2, 251, 147, 3, 588, 52, 65, 9, 8, 194, 624, 28, 63, 5, 19, 1, 4, 444, 8, 194, 643, 22, 19, 611, 1740, 4, 60, 1966, 9, 3106, 22, 6, 392, 4, 6, 417, 392, 390, 7, 43, 918, 164, 1048]),\n",
       "       list([33, 31, 8, 40, 194, 23, 22, 3421, 409, 2798, 78, 77, 2, 6325, 1230, 6810, 100, 1, 11, 6810, 255, 3991, 13, 973, 7, 621, 20, 124, 1608, 5, 2, 5480, 79, 8067, 15, 156, 37, 105, 11, 535, 1025, 24, 1618, 6, 1291, 2044, 15, 2, 3, 2145, 11, 25, 611, 1, 1, 4, 6193, 26, 1796, 6, 316, 1608, 1, 18, 4, 2066, 1957, 4, 6, 5033, 4, 6, 316, 3275, 130, 6, 561, 5762, 4, 6874, 6302, 100, 6810, 234, 10, 649, 897, 1, 73, 27, 445, 3, 11, 1957, 2, 4860, 72, 7, 35, 409, 1406, 1574, 483, 1957, 4006, 48, 4, 1165, 1957, 48, 131, 1041, 2845, 742, 1957, 4, 2730, 1957, 3822, 2672, 2741, 234, 40, 381, 1223, 483]),\n",
       "       list([13, 329, 17, 519, 5872, 9, 6, 369, 1, 53, 5, 21, 184, 6592, 1, 108, 13, 1673, 82, 115, 4, 13, 1673, 135, 2685, 20, 16, 3, 14, 5, 2, 2957, 5246, 1, 17, 4730, 7166, 7700, 572, 53, 14, 703, 18, 1089, 1, 3860, 5, 96, 1, 87, 661, 2, 43, 344, 7, 10, 2916, 3481, 1, 272, 3371, 2154, 6, 3398, 5266, 5, 38, 782, 6, 1, 4730, 172, 4, 8, 7943, 83, 1, 2158, 1478, 2293, 100, 1, 11, 2951, 1, 17, 4839, 1874, 79, 52, 9, 44, 869, 1614, 3, 22, 1772, 9, 8, 3153, 22, 184, 2, 175, 1, 17, 158, 9, 131, 204, 15, 42, 50, 204, 5467, 154, 3263, 14, 3325, 1, 2, 2065, 1, 2, 4571, 22, 6, 27, 3338, 1549, 64, 1349, 5, 61, 55, 2, 2903, 1, 404, 3921, 14, 256, 18, 6, 2453, 1, 11, 3597, 1, 2, 2013, 4680, 8, 1005, 1, 4, 23, 1, 382, 5467, 1489, 55, 15, 8937, 22, 2, 2127, 1, 211, 4, 22, 1396, 181, 6, 3817, 1, 2, 2628, 7, 167, 25, 2504, 6154, 5141, 1, 2]),\n",
       "       ...,\n",
       "       list([39, 9, 10, 12, 335, 173, 995, 30, 9, 40, 1130, 52, 16, 3, 9, 9, 98, 1307, 37, 210, 831, 52, 6, 1159, 18, 2, 2772, 1, 4, 453, 65, 31, 16, 24, 388, 3, 503, 95, 7, 2, 176, 308, 82, 13, 2887, 815, 2, 43, 1, 453, 8, 1003, 14, 609, 3, 9, 6, 657, 91, 1, 312, 1383, 796, 6691, 7, 497, 20, 2245, 3, 2245, 318, 1047, 7122, 23, 1, 53, 523, 30, 1013, 21, 12, 13, 2773, 24, 3, 11, 1307, 37, 210, 4, 720]),\n",
       "       list([21, 349, 31, 93, 1020, 15, 235, 385, 29, 762, 4139, 34, 7, 1054, 1, 11, 886, 78, 23, 13, 72, 4317, 149, 11, 706, 2476, 179, 1, 21, 43, 30, 20, 7411, 13, 628, 4265, 5, 2, 145, 449, 297, 15, 6, 83, 20, 266, 2240, 783, 6, 3375, 8157, 22, 3593, 10, 591, 1, 2024, 1464, 23, 132, 3, 2, 25, 1399, 195, 13, 28, 1714, 5, 410, 237, 223, 7120, 1197, 8716]),\n",
       "       list([21, 9, 10, 12, 647, 110, 67, 7, 73, 15, 235, 559, 186, 8461, 18, 210, 1264, 4, 1068, 5, 157, 2348, 1, 73, 2383, 6352, 4, 6, 164, 143, 250, 268, 8, 64, 123, 5, 21, 12, 17, 1068, 155, 3590, 255, 2642, 4, 7, 16, 339, 1, 2, 12, 13, 9, 10, 12, 1, 186, 1632, 1, 324, 1632, 1837, 23, 31, 10, 12, 40, 4640, 20, 6, 1687, 62, 306, 31, 391, 20, 21, 12, 21, 12, 9, 10, 12, 1, 186, 3558, 5217, 18, 64, 103, 43, 4, 345, 1188, 21, 12, 6, 943, 25, 143, 1, 11, 85, 48, 107, 4, 133, 71, 527, 3, 5, 168, 295, 492, 8, 1904, 172, 3, 14, 28, 4961, 7, 1180, 325, 7, 3, 21, 12, 132, 3, 14, 129, 18, 8, 350, 24, 1456, 2, 103, 4, 2, 436, 255, 66, 24, 2513])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_rnn_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOV_BUCKETS = 5\n",
    "EMBED_SIZE  = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(VOCAB_SIZE + OOV_BUCKETS, EMBED_SIZE, input_shape=[None]),\n",
    "    keras.layers.GRU(4, return_sequences=True),\n",
    "    keras.layers.GRU(4),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn_model = keras.models.Sequential([\n",
    "#     keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "#     dropout=0.2, recurrent_dropout=0.2),\n",
    "#     keras.layers.GRU(128, return_sequences=True,\n",
    "#     dropout=0.2, recurrent_dropout=0.2),\n",
    "#     keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "#     activation=\"softmax\"))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (None, None, 8)           8040      \n",
      "                                                                 \n",
      " gru_24 (GRU)                (None, None, 4)           168       \n",
      "                                                                 \n",
      " gru_25 (GRU)                (None, 4)                 120       \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,333\n",
      "Trainable params: 8,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/547 [============================>.] - ETA: 0s - loss: 0.6386 - accuracy: 0.6140WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "547/547 [==============================] - 23s 30ms/step - loss: 0.6385 - accuracy: 0.6140\n",
      "Epoch 2/5\n",
      "545/547 [============================>.] - ETA: 0s - loss: 0.5745 - accuracy: 0.6937WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "547/547 [==============================] - 15s 28ms/step - loss: 0.5746 - accuracy: 0.6937\n",
      "Epoch 3/5\n",
      "546/547 [============================>.] - ETA: 0s - loss: 0.5677 - accuracy: 0.6967WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.5677 - accuracy: 0.6967\n",
      "Epoch 4/5\n",
      "546/547 [============================>.] - ETA: 0s - loss: 0.5638 - accuracy: 0.7009WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "547/547 [==============================] - 12s 23ms/step - loss: 0.5637 - accuracy: 0.7011\n",
      "Epoch 5/5\n",
      "547/547 [==============================] - ETA: 0s - loss: 0.5603 - accuracy: 0.7039WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "547/547 [==============================] - 17s 30ms/step - loss: 0.5603 - accuracy: 0.7039\n"
     ]
    }
   ],
   "source": [
    "history = rnn_model.fit(x_train_rnn_dense, y_train_rnn, batch_size=64, epochs=5, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 139/1094 [==>...........................] - ETA: 1:44:01 - loss: 0.6936 - accuracy: 0.5067"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m rnn_model\u001b[39m.\u001b[39;49mfit(x_train_rnn_dense, y_train_rnn, epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# history = rnn_model.fit(x_train_rnn_dense, y_train_rnn, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order x_train which is a matrix, order by on the first column and then on the second column\n",
    "x_train_sorted = x_train[np.lexsort((x_train[:,1], x_train[:,0]))]\n",
    "print(x_train_sorted[-30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = rnn_model.fit(x_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1094/1094 [==============================] - 6s 5ms/step - loss: 0.5483 - accuracy: 0.7138\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.5764 - accuracy: 0.6873\n",
      "\n",
      "Training Accuracy: 0.7138000130653381\n",
      "Testing Accuracy: 0.6872666478157043\n"
     ]
    }
   ],
   "source": [
    "train_score = rnn_model.evaluate(x_train_rnn,\n",
    "                       y_train_rnn,\n",
    "                       verbose=1)\n",
    "test_score = rnn_model.evaluate(x_test_rnn,\n",
    "                       y_test_rnn,\n",
    "                       verbose=1)\n",
    "labels = rnn_model.metrics_names\n",
    "\n",
    "print('')\n",
    "print(f'Training Accuracy: {train_score[1]}')\n",
    "print(f'Testing Accuracy: {test_score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 3s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = rnn_model.predict(x_test_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6872666666666667\n",
      "Recall: 0.7048359467650547\n",
      "Precision: 0.6857692307692308\n",
      "f1 score: 0.6951718760153357\n"
     ]
    }
   ],
   "source": [
    "y_pred_binario = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "accuracy = accuracy_score(y_test_rnn, y_pred_binario)\n",
    "recall = recall_score(y_test_rnn, y_pred_binario)\n",
    "f1 = f1_score(y_test_rnn, y_pred_binario,)\n",
    "precision = precision_score(y_test_rnn, y_pred_binario)\n",
    "\n",
    "print(\"Accuracy: \"  + str(accuracy))\n",
    "print(\"Recall: \"    + str(recall))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"f1 score: \"  + str(f1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensamble"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
